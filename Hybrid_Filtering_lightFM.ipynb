{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Filtering \n",
    "\n",
    "### LightFM package with Users Features\n",
    "###### Recent research has demonstrated that hybrid filtering models, combining collaborative and content-based filtering could be more effective in some cases. This method can also overcome some recommendation problems such as the \"cold start\" or \"sparsity\" problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Data \n",
    "df=pd.read_csv(\"C:/Users/alorenzodebrionne/Documents/Python/purchased_data.csv\")\n",
    "users_features = pd.read_csv(\"C:/Users/alorenzodebrionne/Documents/Python/user_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To ensure statistical significance users with less than 5 items, and items with less than 10 ratings are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID: 1112\n",
      "ItemID: 53\n"
     ]
    }
   ],
   "source": [
    "#Filter on Items with have at least 10 users\n",
    "item_count = df.groupby(['ItemID']).size().reset_index(name='counts').sort_values(['counts'], ascending=False)\n",
    "list_items = item_count[item_count['counts']>10]\n",
    "\n",
    "# Filter on Users with at least 5 items\n",
    "user_count = df.groupby(['userID']).size().reset_index(name='counts').sort_values(['counts'], ascending=False)\n",
    "list_users = user_count[user_count['counts']>5]\n",
    "\n",
    "list_items= list_items['ItemID']\n",
    "list_users= list_users['userID']\n",
    "\n",
    "\n",
    "df= df[df['ItemID'].isin(list_items)]\n",
    "df= df[df['userID'].isin(list_users)]\n",
    "\n",
    "users_features = users_features[users_features['userID'].isin(list_users)]\n",
    "\n",
    "\n",
    "print(\"UserID:\", df.userID.nunique())\n",
    "print(\"ItemID:\", df.ItemID.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We convert our User-Item Matrix to binary matrix. NB: you can create a Weighted Matrix and use it during the LightFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot(index='userID', columns='ItemID').nb_purchased\n",
    "userID = pd.DataFrame(df_pivot.index)\n",
    "# Replace all zeros per NaN\n",
    "df_pivot=df_pivot.fillna(0)\n",
    "# Binary Interactions \n",
    "df_pivot[df_pivot != 0] =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform your Matrices to a compress a sparse row format\n",
    "##### Users Features & Items Features needs to contain ONLY NUMERICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count number of Products and Users\n",
    "n_items= len(df_pivot.columns)\n",
    "n_users =len(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_features=pd.get_dummies(users_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_item_matrix= sparse.csr_matrix(df_pivot.values)\n",
    "user_property_matrix = sparse.csr_matrix(users_features.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train set and test set\n",
    "### Process for Data Masking\n",
    "##### Randomly assign 0 values to the training set - Test set is a copy of the DataFrame\n",
    "###### Keep the list of users that were altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def make_train(ratings, pct_test = 0.25):\n",
    "    \n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(123) # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, item_users_altered = make_train(user_to_item_matrix, pct_test = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.9428374767303467 at {'learning_rate': 0.028546874915723632, 'user_alpha': 9.339008221479138e-10, 'loss': 'warp', 'num_sample_hyperparametersepochs': 45, 'item_alpha': 5.2037162041155225e-09, 'learning_schedule': 'adadelta', 'max_sampled': 5, 'no_components': 59}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object random_search at 0x000001BD2CE7B1A8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Yield possible hyperparameter choices.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        yield {\n",
    "            \"no_components\": np.random.randint(16, 64),\n",
    "            \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n",
    "            \"loss\": np.random.choice([\"bpr\", \"warp\", \"warp-kos\"]),\n",
    "            \"learning_rate\": np.random.exponential(0.05),\n",
    "            \"item_alpha\": np.random.exponential(1e-8),\n",
    "            \"user_alpha\": np.random.exponential(1e-8),\n",
    "            \"max_sampled\": np.random.randint(5, 15),\n",
    "            \"num_epochs\": np.random.randint(5, 50),\n",
    "        }\n",
    "\n",
    "\n",
    "def random_search(train, test, num_samples=10, num_threads=1):\n",
    "\n",
    "    for hyperparams in itertools.islice(sample_hyperparameters(), num_samples):\n",
    "        num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "\n",
    "        model = LightFM(**hyperparams)\n",
    "        model.fit(train, epochs=num_epochs)\n",
    "\n",
    "        score = auc_score(model, X_test).mean()\n",
    "\n",
    "        hyperparams[\"num_sample_hyperparametersepochs\"] = num_epochs\n",
    "\n",
    "        yield (score, hyperparams, model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    (score, hyperparams, model) = max(random_search(X_train, X_test, num_threads=2), key=lambda x: x[0])\n",
    "\n",
    "    print(\"Best score {} at {}\".format(score, hyperparams))\n",
    "\n",
    "random_search(X_train,X_test,num_samples=10, num_threads=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x1bd2cf14f98>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters optimized\n",
    "no_comp, lr, ep, max_sam = 59, 0.01746475831477904,45 , 5\n",
    "ua, ia = 9.540255960433563e-09, 6.794510538622656e-09\n",
    "\n",
    "model=LightFM(no_components=no_comp,learning_rate=lr, loss='warp', learning_schedule = 'adadelta', max_sampled = max_sam, user_alpha = ua, item_alpha = ia)\n",
    "\n",
    "model.fit(X_train,epochs=ep,verbose=True,user_features=user_property_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions : http://lyst.github.io/lightfm/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
